{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Science Portfolio (Part2)\n"],"metadata":{"id":"oJ34uxMILElC"}},{"cell_type":"code","source":["import numpy as np\n","import string\n","import pandas as pd\n","import re\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.graph_objects as go\n","import plotly.express as px\n","import math\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","# the code block below is directly downloading commentary.txt and superheros.csv into your drive folder. Please just run it and do not comment out.\n","from urllib import request\n","module_url = [f\"https://drive.google.com/uc?export=view&id=18y6hLv2bqAyJsIXwVCty58lF0u7yimVq\"]\n","name = ['commentary.txt']\n","for i in range(len(name)):\n","    with request.urlopen(module_url[i]) as f, open(name[i],'w') as outf:\n","        a = f.read()\n","        outf.write(a.decode('ISO-8859-1'))\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk import pos_tag\n","import nltk\n","import re\n","from tqdm import tqdm\n","tqdm.pandas()\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","nltk.download('wordnet')"],"metadata":{"id":"7PRe1FadWJR-","executionInfo":{"status":"ok","timestamp":1715341110534,"user_tz":-60,"elapsed":8790,"user":{"displayName":"Jiaan Li","userId":"17657632301940103772"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"98a9de6f-3af9-48fb-8700-d81c347c6f24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["#Text Analysis\n","\n","In this question, we will interrogate the football commentary dataset"],"metadata":{"id":"gxiHUV1sdEjU"}},{"cell_type":"code","source":["df = pd.read_csv('commentary.txt', sep='\\t')\n","df.head()"],"metadata":{"id":"ej3H78ubdEjV","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1715007209163,"user_tz":-60,"elapsed":384,"user":{"displayName":"Jiaan Li","userId":"17657632301940103772"}},"outputId":"5897771f-17f1-4934-b906-a01d191ad37a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Minute                                         Commentary\n","0      97  Plenty of chances in this game but neither tea...\n","1      97     That's it! The referee blows the final whistle\n","2      97   Ball possession: Tottenham: 44%, Liverpool: 56%.\n","3      96  James Milner relieves the pressure with a clea...\n","4      96  Poor play by Trent Alexander-Arnold as his wea..."],"text/html":["\n","  <div id=\"df-34030671-77c3-4473-8c2b-d9f31bf2a732\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Minute</th>\n","      <th>Commentary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>97</td>\n","      <td>Plenty of chances in this game but neither tea...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>97</td>\n","      <td>That's it! The referee blows the final whistle</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>97</td>\n","      <td>Ball possession: Tottenham: 44%, Liverpool: 56%.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>96</td>\n","      <td>James Milner relieves the pressure with a clea...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>96</td>\n","      <td>Poor play by Trent Alexander-Arnold as his wea...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34030671-77c3-4473-8c2b-d9f31bf2a732')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-34030671-77c3-4473-8c2b-d9f31bf2a732 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-34030671-77c3-4473-8c2b-d9f31bf2a732');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-f6ef6491-d21b-48c1-ab0e-2f43b6da2bfc\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f6ef6491-d21b-48c1-ab0e-2f43b6da2bfc')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-f6ef6491-d21b-48c1-ab0e-2f43b6da2bfc button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 31717,\n  \"fields\": [\n    {\n      \"column\": \"Minute\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 27,\n        \"min\": 1,\n        \"max\": 104,\n        \"num_unique_values\": 104,\n        \"samples\": [\n          67,\n          32,\n          33\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Commentary\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 11917,\n        \"samples\": [\n          \"Danny Rose heads the ball back across goal...\",\n          \"Poor play by Jamie Vardy as his weak attempt to clear the ball puts his side under pressure\",\n          \"Could be a good chance here as Cristiano Ronaldo from Manchester United cuts the opponents defense open with a through ball...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["## Step 1 Preprocessing\n","\n","Implement a method for obtaining tokenized, PoS-tagged and PoS-tagged and lemmatized versions of the Commentary column. Create 3 new columns: `Tokenized`, `PoS_tagged` and `PoS_lemmatized`, and create them in order:\n","\n","1.- New `Tokenized` column, by lower casing and tokenizing the `Commentary` column.\n","\n","2.- New `PoS_tagged` column, by pos_tagging the `Tokenized` column.\n","\n","3.- New `PoS_lemmatized` column, by lemmatizing only the words in the `PoS_tagged` column. The reason for doing it in this order is to present to the tagging function the original text.\n","\n"],"metadata":{"id":"lM-Hl3i69JrI"}},{"cell_type":"code","source":["df = pd.read_csv('commentary.txt', sep='\\t')\n","\n","# Tokenize the Commentary column and create Tokenized column\n","df['Tokenized'] = df['Commentary'].apply(lambda x: word_tokenize(x.lower()))\n","\n","# PoS tagging on the Tokenized column and create PoS_tagged column\n","df['PoS_tagged'] = df['Tokenized'].apply(lambda x: pos_tag(x))\n","# Initialize WordNet lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# Map NLTK PoS tags to WordNet PoS tags\n","def map_pos_tag(nltk_tag):\n","    if nltk_tag.startswith('J'):\n","        return 'a'  # Adjective\n","    elif nltk_tag.startswith('V'):\n","        return 'v'  # Verb\n","    elif nltk_tag.startswith('N'):\n","        return 'n'  # Noun\n","    elif nltk_tag.startswith('R'):\n","        return 'r'  # Adverb\n","    else:\n","        return None  # None by default, can be adjusted as needed\n","\n","# Function to lemmatize words based on PoS tags\n","def lemmatize_word(word, pos_tag):\n","    pos_tag_original = pos_tag\n","    pos_tag = map_pos_tag(pos_tag)\n","    if pos_tag:\n","        return (lemmatizer.lemmatize(word, pos_tag), pos_tag_original)\n","    else:\n","        return (lemmatizer.lemmatize(word),pos_tag_original)\n","\n","# Lemmatize words in PoS_tagged column and create PoS_lemmatized column\n","df['PoS_lemmatized'] = df['PoS_tagged'].apply(lambda x: [lemmatize_word(word, pos_tag) for word, pos_tag in x])\n"],"metadata":{"id":"dBFF172_a_-Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[">>print(df['Tokenized'][:3])\n","0    [plenty, of, chances, in, this, game, but, nei...\n","1    [that, 's, it, !, the, referee, blows, the, fi...\n","2    [ball, possession, :, tottenham, :, 44, %, ,, ...\n","Name: Tokenized, dtype: object\n","\n",">>print(df['PoS_tagged'][:3])\n","0    [(plenty, NN), (of, IN), (chances, NNS), (in, ...\n","1    [(that, DT), ('s, VBZ), (it, PRP), (!, .), (th...\n","2    [(ball, DT), (possession, NN), (:, :), (totten...\n","Name: PoS_tagged, dtype: object\n","\n",">>print(df['PoS_lemmatized'][:3])\n","0    [(plenty, NN), (of, IN), (chance, NNS), (in, I...\n","1    [(that, DT), ('s, VBZ), (it, PRP), (!, .), (th...\n","2    [(ball, DT), (possession, NN), (:, :), (totten...\n","Name: PoS_lemmatized, dtype: object"],"metadata":{"id":"nHuLdKb3Tbe1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df['Tokenized'][:3])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CzZ87TLSTek7","executionInfo":{"status":"ok","timestamp":1715341255927,"user_tz":-60,"elapsed":237,"user":{"displayName":"Jiaan Li","userId":"17657632301940103772"}},"outputId":"87661304-a751-4187-a766-004b3a389ea7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0    [plenty, of, chances, in, this, game, but, nei...\n","1    [that, 's, it, !, the, referee, blows, the, fi...\n","2    [ball, possession, :, tottenham, :, 44, %, ,, ...\n","Name: Tokenized, dtype: object\n"]}]},{"cell_type":"code","source":["print(df['PoS_tagged'][:3])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m5vsCgvKThGS","executionInfo":{"status":"ok","timestamp":1715341257563,"user_tz":-60,"elapsed":203,"user":{"displayName":"Jiaan Li","userId":"17657632301940103772"}},"outputId":"90f8a4c9-17a7-42f8-8848-4ca3ff3d954b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0    [(plenty, NN), (of, IN), (chances, NNS), (in, ...\n","1    [(that, DT), ('s, VBZ), (it, PRP), (!, .), (th...\n","2    [(ball, DT), (possession, NN), (:, :), (totten...\n","Name: PoS_tagged, dtype: object\n"]}]},{"cell_type":"code","source":["print(df['PoS_lemmatized'][:3])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A7GhKTBtTibO","executionInfo":{"status":"ok","timestamp":1715341259171,"user_tz":-60,"elapsed":200,"user":{"displayName":"Jiaan Li","userId":"17657632301940103772"}},"outputId":"d3ab3587-0811-4e7a-d92a-06db49327d50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0    [(plenty, NN), (of, IN), (chance, NNS), (in, I...\n","1    [(that, DT), ('s, VBZ), (it, PRP), (!, .), (th...\n","2    [(ball, DT), (possession, NN), (:, :), (totten...\n","Name: PoS_lemmatized, dtype: object\n"]}]},{"cell_type":"markdown","source":["## Step 2 Basic search engine\n","\n","Implement a basic search engine in a function called `retrieve_similar_commentaries(df, query, k)`, which takes as input the following arguments:\n","\n","- `df` the previously enriched (tokenized, pos tagged, etc) commentary dataframe.\n","- `query` a string of any type, which will be the query we will be using to retrieve similar commentaries.\n","- `k` and integer denoting the top `k` commentaries to be returned (by similarity).\n","\n","Function performs the following steps:\n","\n","1 - Tokenize and lemmatize the input query.\n","\n","2 - For each commentary in the df, compute how similar it is to the query as the number of shared tokens between query and commentary.\n","\n","3 - We will prioritize noun matches, so our similarity score will receive +1 if at least one of the matching tokens in the commentary is a noun (i.e., its part of speech starts with `N`). This means that, for example, if your query has 2 tokens, the maximum similarity a commentary can have is 4: 2 for 2 overlapping tokens, and 2 for both tokens being nouns.\n","\n","4 - The function must return a list of tuples of the form `[(commentary1, sim), (commentary2, sim) ... (commentaryk, sim)]`, where commentaries are ranked by `sim` value in descending order.\n","\n","\n","```"],"metadata":{"id":"IYsMTNgJWO-t"}},{"cell_type":"code","source":["def retrieve_similar_commentaries(df, query, k):\n","    \"\"\"\n","    Retrieve the top k commentaries from a DataFrame that are most similar to the given query.\n","\n","    Args:\n","        df (DataFrame): The DataFrame containing commentaries and their attributes.\n","        query (str): The query for which similar commentaries are to be retrieved.\n","        k (int): The number of similar commentaries to retrieve.\n","\n","    Returns:\n","        list: A list of tuples containing the top k similar commentaries and their similarity scores.\n","\n","    Note:\n","        This function assumes that the DataFrame `df` has columns named 'Tokenized' and 'PoS_tagged',\n","        which contain preprocessed tokens and part-of-speech tagged tokens for each commentary respectively.\n","        The 'Tokenized' column should contain tokenized and lemmatized versions of the commentaries.\n","        The 'PoS_tagged' column should contain part-of-speech tagged tokens of the commentaries.\n","    \"\"\"\n","    # Initialize WordNet lemmatizer\n","    lemmatizer = WordNetLemmatizer()\n","    # Tokenize and lemmatize the input query\n","    query_tokens = [lemmatizer.lemmatize(token.lower()) for token in word_tokenize(query)]\n","\n","    # Initialize list to store similarity scores\n","    similarity_scores = []\n","\n","    # Iterate through each commentary in the dataframe\n","    for index, row in df.iterrows():\n","        # Tokenize and lemmatize the commentary\n","        commentary_tokens = row['Tokenized']\n","\n","        # Compute similarity as the number of shared tokens between query and commentary\n","        shared_tokens = set(query_tokens) & set(commentary_tokens)\n","\n","        # Initialize similarity score\n","        similarity_score = len(shared_tokens)\n","        tokens = []\n","        # # Check if at least one of the matching tokens in the commentary is a noun\n","        # if any(token[1].startswith('N') for token in row['PoS_tagged'] if token[0] in shared_tokens):\n","        #     similarity_score += 1\n","        for token in row['PoS_tagged']:\n","          if (token[0] in shared_tokens) and (token[1].startswith('N')) and (token not in tokens):\n","            similarity_score += 1\n","            tokens.append(token)\n","\n","        # Append (commentary, similarity) tuple to list\n","        similarity_scores.append((row['Commentary'], similarity_score))\n","\n","    # Sort similarity scores in descending order\n","    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n","\n","    # Return top k commentaries\n","    return similarity_scores[:k]"],"metadata":{"id":"erbMhFPjpwoJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = retrieve_similar_commentaries(df, \"Manchester United ball\", 3)\n","for idx,r in enumerate(result):\n","  print(idx,r)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dQ2qh5Y0Tu8R","executionInfo":{"status":"ok","timestamp":1715341304350,"user_tz":-60,"elapsed":2276,"user":{"displayName":"Jiaan Li","userId":"17657632301940103772"}},"outputId":"e6850fcc-3023-43cd-eb5e-0d43d7edacb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 ('Manchester United is in control of the ball.', 5)\n","1 ('Manchester United is in control of the ball.', 5)\n","2 ('Jadon Sancho from Manchester United crosses the ball, but it goes out for a goal kick.', 5)\n"]}]},{"cell_type":"markdown","source":["##Step3  - PMI\n","Implement and apply the pointwise mutual information (PMI) metric, a word association metric introduced in 1992, to the football commentaries. The purpose of PMI is to extract, from free text, pairs of words or phrases than tend to co-occur together more often than expected by chance. For example, PMI(`new`, `york`) would give a higher score than PMI(`new`, `car`) because the chance of finding `new` and `york` together in text is higher than `new` and `car`, despite `new` being a more frequent word than `york`.\n","\n","The formula for PMI (where `x` and `y` are two words) is:\n","\n","$PMI(x,y) = log(\\frac{p(x,y)}{p(x)p(y)})$\n"],"metadata":{"id":"xdbA8XlR9j6P"}},{"cell_type":"markdown","source":["\n","\n","- **Phrase Extraction**: The first step is to extract noun phrases (NPs) and verb phrases (VPs) from the lemmatized data. We will reward cases where NPs and VPs go beyond single word matching.\n","\n","- **Phrase Counting**: Once have extracted the NPs and VPs, you'll need to count how many times each phrase occurs in the dataset.\n","\n","- **Total Counts**: The next step is to compute the total count of all NPs and VPs. This is simply the sum of all the counts in the dictionaries created in the previous step.\n","\n","- **Identifying Top Phrases**: To reduce computational complexity, we only want to compute PMI for the top occurring NPs and VPs.\n","- **Creating the PMI Matrix**: Finally, create a PMI matrix using the top NPs and VPs, their counts, and the total counts of NPs and VPs. This matrix is a pandas DataFrame, which will have rows corresponding to the top VPs, columns corresponding to the top NPs, and each cell will contain the PMI value between the corresponding NP and VP."],"metadata":{"id":"zjwmwOLHMyB9"}},{"cell_type":"code","source":["def extract_phrases(tagged_tokens):\n","    \"\"\"\n","    Extracts noun phrases (NP) and verb phrases (VP) from tagged tokens.\n","\n","    Args:\n","    tagged_tokens (list): A list of tuples containing (word, tag) pairs.\n","\n","    Returns:\n","    tuple: A tuple containing two lists, the first for noun phrases and the second for verb phrases.\n","    \"\"\"\n","    # Define the grammar for noun phrases (NP) and verb phrases (VP)\n","    grammar = r\"\"\"\n","        NP: {<DT>?<JJ>*<NN>*} # NP\n","        VP: {<VB.*><DT>?<JJ>*<NN|NNS>*} # VP\n","    \"\"\"\n","    # Create a chunk parser using the defined grammar\n","    cp = nltk.RegexpParser(grammar)\n","    # Parse the tagged tokens\n","    parsed_tree = cp.parse(tagged_tokens)\n","    # Initialize lists to store NP and VP phrases\n","    np_phrases = []\n","    vp_phrases = []\n","\n","    # Traverse the parsed tree and extract NP and VP phrases\n","    for subtree in parsed_tree.subtrees():\n","        if subtree.label() == 'NP':\n","            np_phrases.append(' '.join([token[0] for token in subtree.leaves()]))\n","        elif subtree.label() == 'VP':\n","            vp_phrases.append(' '.join([token[0] for token in subtree.leaves()]))\n","\n","\n","    return np_phrases, vp_phrases\n","\n","\n","def count_phrases_in_df(df, phrases):\n","    \"\"\"\n","    Count occurrences of each NP and VP in the 'commentary' column of the DataFrame.\n","\n","    Args:\n","    df (DataFrame): A pandas DataFrame containing the 'commentary' column.\n","    phrases (dict): A dictionary containing lists of noun phrases (nps) and verb phrases (vps).\n","\n","    Returns:\n","    tuple: A tuple containing two nltk.FreqDist objects, the first for NP counts and the second for VP counts.\n","    \"\"\"\n","    # Create FreqDist objects to count occurrences of NP and VP phrases\n","    np_counts = nltk.FreqDist(phrases['nps'])\n","    vp_counts = nltk.FreqDist(phrases['vps'])\n","    return np_counts, vp_counts\n","\n","def merge_and_get_top_phrases(np_counts, vp_counts):\n","    \"\"\"\n","    Merge NP and VP counts, calculate the top 100 most frequent phrases,\n","    and organize them into a dictionary.\n","\n","    Args:\n","    np_counts (nltk.FreqDist): A FreqDist object containing counts of NP phrases.\n","    vp_counts (nltk.FreqDist): A FreqDist object containing counts of VP phrases.\n","\n","    Returns:\n","    dict: A dictionary containing lists of the top 100 most frequent NP and VP phrases.\n","    \"\"\"\n","    # Get the top 100 most common NP and VP phrases\n","    top_nps = np_counts.most_common(100)\n","    top_vps = vp_counts.most_common(100)\n","    # Initialize a dictionary to store the top NP and VP phrases\n","    top_phrases_dict = {'nps': [], 'vps': []}\n","\n","    # Store the top NP phrases in the dictionary\n","    for phrase, count in top_nps:\n","        top_phrases_dict['nps'].append((phrase, count))\n","    # Store the top VP phrases in the dictionary\n","    for phrase, count in top_vps:\n","        top_phrases_dict['vps'].append((phrase, count))\n","    return top_phrases_dict\n","\n","def compute_pmi_dataframe(df):\n","    \"\"\"\n","    Compute Pointwise Mutual Information (PMI) between noun phrases (NPs) and verb phrases (VPs)\n","    based on the commentary in the DataFrame.\n","\n","    Args:\n","    df (DataFrame): A pandas DataFrame containing 'PoS_tagged' and 'Commentary' columns.\n","\n","    Returns:\n","    DataFrame: A DataFrame containing PMI values for each NP-VP pair.\n","    \"\"\"\n","    commentary_corpus = []\n","    stop_words = {',', 'the', 'a', 'that', 's', '%'}\n","    phrases = {'nps': [], 'vps': []}\n","\n","    # Extract NPs and VPs from each PoS-tagged commentary\n","    for x in df['PoS_tagged']:\n","        np_phrases, vp_phrases = extract_phrases(x)\n","        phrases['nps'].extend(np_phrases)\n","        phrases['vps'].extend(vp_phrases)\n","\n","    # Filter out stop words from NP phrases\n","    phrases['nps'] = [phrase_tuple for phrase_tuple in phrases['nps'] if phrase_tuple[0].lower() not in stop_words]\n","    # Count occurrences of NP and VP phrases in the DataFrame\n","    np_counts, vp_counts = count_phrases_in_df(df, phrases)\n","    # Merge counts and get top phrases\n","    top_phrases_dict = merge_and_get_top_phrases(np_counts, vp_counts)\n","\n","    nps = [phrase for phrase, _ in top_phrases_dict['nps']]\n","    vps = [phrase for phrase, _ in top_phrases_dict['vps']]\n","\n","\n","    # Iterate through each commentary in the 'commentary' column\n","    for commentary in df['Commentary']:\n","        # Convert the commentary to lowercase and append it to the corpus list\n","        commentary_corpus.append(commentary.lower())\n","\n","    # Initialize a co-occurrence matrix\n","    co_occurrence_matrix = np.zeros((len(vps), len(nps)), dtype=int)\n","\n","    # Tokenize and preprocess sentences, and extract NPs and VPs\n","    for sentence in commentary_corpus:\n","        # Update co-occurrence matrix for each sentence\n","        for np_index, np_phrase in enumerate(nps):\n","            for vp_index, vp_phrase in enumerate(vps):\n","                if np_phrase in sentence and vp_phrase in sentence:\n","                    # Increment co-occurrence count for this NP-VP pair\n","                    co_occurrence_matrix[vp_index, np_index] += 1\n","    #Add 2 smoothing\n","    co_occurrence_matrix += 2\n","    # Calculate total count\n","    total_count = np.sum(co_occurrence_matrix)\n","\n","    # Calculate probabilities\n","    p_np = np.sum(co_occurrence_matrix, axis=0) / total_count  # Probability of each NP\n","    p_vp = np.sum(co_occurrence_matrix, axis=1) / total_count  # Probability of each VP\n","    p_np_vp = co_occurrence_matrix / total_count  # Joint probability of each NP-VP pair\n","\n","    # Compute PMI\n","    pmi_matrix = np.log(p_np_vp / np.outer(p_vp, p_np))\n","\n","    # Replace negative PMI values with 0\n","    pmi_matrix = np.maximum(pmi_matrix, 0)\n","\n","    # Create a DataFrame for PMI values\n","    pmi_df = pd.DataFrame(pmi_matrix, index=vps, columns=nps)\n","\n","\n","    return pmi_df"],"metadata":{"id":"soKNXNg8az0a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","pmidf = compute_pmi_dataframe(df)"],"metadata":{"id":"nwjShxK2RpGo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# you can test your resulting matrix\n","def top_k_vps(pmi_matrix, np, k):\n","    # Check if the NP exists in the matrix\n","    if np in pmi_matrix.T.index:\n","        top_vps = pmi_matrix.T.loc[np].nlargest(k)\n","        return top_vps.index.tolist()\n","    else:\n","        print(f\"Noun phrase '{np}' not found in PMI matrix.\")\n","        return []\n","top_k_vps(pmidf, 'joao cancelo', 3)"],"metadata":{"id":"fo9ahEkyJlH2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715341246418,"user_tz":-60,"elapsed":240,"user":{"displayName":"Jiaan Li","userId":"17657632301940103772"}},"outputId":"288fd2f9-5d3c-48ee-fcb1-c4a09d3e8ba8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['stops', 'parried', 'puts']"]},"metadata":{},"execution_count":7}]}]}